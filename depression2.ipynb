{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b4acb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:14.957776Z",
     "iopub.status.busy": "2022-04-10T14:05:14.956915Z",
     "iopub.status.idle": "2022-04-10T14:05:22.974797Z",
     "shell.execute_reply": "2022-04-10T14:05:22.974156Z",
     "shell.execute_reply.started": "2022-04-10T14:02:58.774889Z"
    },
    "papermill": {
     "duration": 8.065416,
     "end_time": "2022-04-10T14:05:22.974954",
     "exception": false,
     "start_time": "2022-04-10T14:05:14.909538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import transformers\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845856fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:23.041547Z",
     "iopub.status.busy": "2022-04-10T14:05:23.041030Z",
     "iopub.status.idle": "2022-04-10T14:05:23.218680Z",
     "shell.execute_reply": "2022-04-10T14:05:23.219099Z",
     "shell.execute_reply.started": "2022-04-10T14:02:58.790426Z"
    },
    "papermill": {
     "duration": 0.212181,
     "end_time": "2022-04-10T14:05:23.219253",
     "exception": false,
     "start_time": "2022-04-10T14:05:23.007072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Waiting for my mind to have a breakdown once t...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My new years resolution : I'm gonna get my ass...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New year : Somone else Feeling like 2020 will ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My story I guess : Hi, Im from Germany and my ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat in the dark and cried myself going into th...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text_data     Label\n",
       "0  Waiting for my mind to have a breakdown once t...  moderate\n",
       "1  My new years resolution : I'm gonna get my ass...  moderate\n",
       "2  New year : Somone else Feeling like 2020 will ...  moderate\n",
       "3  My story I guess : Hi, Im from Germany and my ...  moderate\n",
       "4  Sat in the dark and cried myself going into th...  moderate"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('../input/depression/depression_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdb24bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:23.310724Z",
     "iopub.status.busy": "2022-04-10T14:05:23.309833Z",
     "iopub.status.idle": "2022-04-10T14:05:32.772670Z",
     "shell.execute_reply": "2022-04-10T14:05:32.772156Z",
     "shell.execute_reply.started": "2022-04-10T14:02:58.883223Z"
    },
    "papermill": {
     "duration": 9.519611,
     "end_time": "2022-04-10T14:05:32.772817",
     "exception": false,
     "start_time": "2022-04-10T14:05:23.253206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>waiting mind breakdown feeling anymore know an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new year resolution gonna get as therapist off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new year somone else feeling like last year ea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>story guess hi im germany english mostly self ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sat dark cried going new year great start</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text_data  Label\n",
       "0  waiting mind breakdown feeling anymore know an...      1\n",
       "1  new year resolution gonna get as therapist off...      1\n",
       "2  new year somone else feeling like last year ea...      1\n",
       "3  story guess hi im germany english mostly self ...      1\n",
       "4          sat dark cried going new year great start      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "wl = WordNetLemmatizer()\n",
    "mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
    "           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n",
    "           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
    "           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n",
    "           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "           \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "#function to clean data\n",
    "def clean_text(text,lemmatize = True):\n",
    "    soup = BeautifulSoup(text, \"html.parser\") #remove html tags\n",
    "    text = soup.get_text()\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #expanding chatwords and contracts clearing contractions\n",
    "    emoji_clean= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_clean.sub(r'',text)\n",
    "    text = re.sub(r'\\.(?=\\S)', '. ',text) #add space after full stop\n",
    "    text = re.sub(r'http\\S+', '', text) #remove urls\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    #tokens = re.split('\\W+', text) #create tokens\n",
    "    if lemmatize:\n",
    "        text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n",
    "    else:\n",
    "        text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n",
    "    return text\n",
    "data_copy = data.copy()\n",
    "data['Text_data']=data['Text_data'].apply(clean_text,lemmatize = True)\n",
    "#converting target variable to numeric labels\n",
    "labeling = {'moderate':1, 'severe':2, 'not depression':0}\n",
    "data['Label'] = data['Label'].apply(lambda x : labeling[x])\n",
    "#data.label = [ 1 if each == \"positive\" 0 else if each==\"not depression\" else 2 for each in data.Text_data]\n",
    "#after converting labels\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f2d456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:33.148037Z",
     "iopub.status.busy": "2022-04-10T14:05:33.077178Z",
     "iopub.status.idle": "2022-04-10T14:05:37.920419Z",
     "shell.execute_reply": "2022-04-10T14:05:37.919428Z",
     "shell.execute_reply.started": "2022-04-10T14:03:06.484138Z"
    },
    "papermill": {
     "duration": 5.113583,
     "end_time": "2022-04-10T14:05:37.920557",
     "exception": false,
     "start_time": "2022-04-10T14:05:32.806974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#splitting into train and test\n",
    "data_copy['Text_data']=data_copy['Text_data'].apply(clean_text,lemmatize = False)\n",
    "#converting target variable to numerical value\n",
    "#labeling = {'moderate':1, 'severe':2, 'not depression':0}\n",
    "#test_data['Label'] = test_data['Label'].apply(lambda x : labeling[x])\n",
    "data_copy.sentiment = [ {'moderate':1, 'severe':2, 'not depression':0} for each in data_copy.Label]\n",
    "train, test= train_test_split(data_copy, test_size=0.2, random_state=42)\n",
    "Xtrain, ytrain = train['Text_data'], train['Label']\n",
    "Xtest, ytest = test['Text_data'], test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa20e244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:38.007189Z",
     "iopub.status.busy": "2022-04-10T14:05:38.001130Z",
     "iopub.status.idle": "2022-04-10T14:05:38.351864Z",
     "shell.execute_reply": "2022-04-10T14:05:38.352577Z",
     "shell.execute_reply.started": "2022-04-10T14:03:10.932342Z"
    },
    "papermill": {
     "duration": 0.396838,
     "end_time": "2022-04-10T14:05:38.352787",
     "exception": false,
     "start_time": "2022-04-10T14:05:37.955949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of the dataset is :  12037\n"
     ]
    }
   ],
   "source": [
    "#set up the tokenizer\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "word_index = tokenizer.word_index\n",
    "#print(word_index)\n",
    "V = len(word_index)\n",
    "print(\"Vocabulary of the dataset is : \",V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf8b753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:38.484778Z",
     "iopub.status.busy": "2022-04-10T14:05:38.483779Z",
     "iopub.status.idle": "2022-04-10T14:05:39.071886Z",
     "shell.execute_reply": "2022-04-10T14:05:39.071290Z",
     "shell.execute_reply.started": "2022-04-10T14:03:11.305499Z"
    },
    "papermill": {
     "duration": 0.657516,
     "end_time": "2022-04-10T14:05:39.072009",
     "exception": false,
     "start_time": "2022-04-10T14:05:38.414493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequence in the list: 1385\n"
     ]
    }
   ],
   "source": [
    "##create sequences of reviews\n",
    "seq_train = tokenizer.texts_to_sequences(Xtrain)\n",
    "seq_test =  tokenizer.texts_to_sequences(Xtest)\n",
    "#choice of maximum length of sequences\n",
    "seq_len_list = [len(i) for i in seq_train + seq_test]\n",
    "\n",
    "#if we take the direct maximum then\n",
    "max_len=max(seq_len_list)\n",
    "print('Maximum length of sequence in the list: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e858edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:39.155966Z",
     "iopub.status.busy": "2022-04-10T14:05:39.155001Z",
     "iopub.status.idle": "2022-04-10T14:05:39.158892Z",
     "shell.execute_reply": "2022-04-10T14:05:39.159459Z",
     "shell.execute_reply.started": "2022-04-10T14:03:11.651649Z"
    },
    "papermill": {
     "duration": 0.049543,
     "end_time": "2022-04-10T14:05:39.159643",
     "exception": false,
     "start_time": "2022-04-10T14:05:39.110100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of the sequence when considering data only two standard deviations from average: 239\n"
     ]
    }
   ],
   "source": [
    "# when setting the maximum length of sequence, variability around the average is used.\n",
    "max_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\n",
    "max_seq_len = int(max_seq_len)\n",
    "print('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc7166a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:39.238943Z",
     "iopub.status.busy": "2022-04-10T14:05:39.238023Z",
     "iopub.status.idle": "2022-04-10T14:05:39.245392Z",
     "shell.execute_reply": "2022-04-10T14:05:39.245899Z",
     "shell.execute_reply.started": "2022-04-10T14:03:11.661518Z"
    },
    "papermill": {
     "duration": 0.049056,
     "end_time": "2022-04-10T14:05:39.246039",
     "exception": false,
     "start_time": "2022-04-10T14:05:39.196983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The above calculated number coveres approximately 95.87 % of data\n"
     ]
    }
   ],
   "source": [
    "perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) / len(seq_len_list)*100\n",
    "print('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339978f9",
   "metadata": {
    "papermill": {
     "duration": 0.03754,
     "end_time": "2022-04-10T14:05:39.321621",
     "exception": false,
     "start_time": "2022-04-10T14:05:39.284081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e10647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:39.406736Z",
     "iopub.status.busy": "2022-04-10T14:05:39.405811Z",
     "iopub.status.idle": "2022-04-10T14:05:40.591168Z",
     "shell.execute_reply": "2022-04-10T14:05:40.591594Z",
     "shell.execute_reply.started": "2022-04-10T14:03:11.673139Z"
    },
    "papermill": {
     "duration": 1.232219,
     "end_time": "2022-04-10T14:05:40.591740",
     "exception": false,
     "start_time": "2022-04-10T14:05:39.359521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_Text_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Waiting for my mind to have a breakdown once t...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>waiting mind breakdown “new year” feeling isn’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My new years resolution : I'm gonna get my ass...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>new years resolution im gonna get ass therapis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New year : Somone else Feeling like 2020 will ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>new year somone else feeling like 2020 last ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My story I guess : Hi, Im from Germany and my ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>story guess hi im germany english mostly self ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat in the dark and cried myself going into th...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>sat dark cried going new year great start 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text_data     Label  \\\n",
       "0  Waiting for my mind to have a breakdown once t...  moderate   \n",
       "1  My new years resolution : I'm gonna get my ass...  moderate   \n",
       "2  New year : Somone else Feeling like 2020 will ...  moderate   \n",
       "3  My story I guess : Hi, Im from Germany and my ...  moderate   \n",
       "4  Sat in the dark and cried myself going into th...  moderate   \n",
       "\n",
       "                                   cleaned_Text_data  \n",
       "0  waiting mind breakdown “new year” feeling isn’...  \n",
       "1  new years resolution im gonna get ass therapis...  \n",
       "2  new year somone else feeling like 2020 last ye...  \n",
       "3  story guess hi im germany english mostly self ...  \n",
       "4     sat dark cried going new year great start 2020  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df = pd.read_csv('../input/depression/depression_train.csv')\n",
    "#Data Processing — convert to lower case, Remove punctuation etc\n",
    "def data_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text) # Remove HTML from text\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_Text_data'] = df['Text_data'].apply(data_preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea4153b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:40.667046Z",
     "iopub.status.busy": "2022-04-10T14:05:40.661792Z",
     "iopub.status.idle": "2022-04-10T14:05:40.791424Z",
     "shell.execute_reply": "2022-04-10T14:05:40.791846Z",
     "shell.execute_reply.started": "2022-04-10T14:03:12.877768Z"
    },
    "papermill": {
     "duration": 0.166295,
     "end_time": "2022-04-10T14:05:40.791982",
     "exception": false,
     "start_time": "2022-04-10T14:05:40.625687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waiting', 'mind', 'breakdown', 'feeling', 'anymore']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_corpus(text):\n",
    "    words = []\n",
    "    for i in text:\n",
    "        for j in i.split():\n",
    "            words.append(j.strip())\n",
    "    return words\n",
    "corpus = get_corpus(data.Text_data)\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13271cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:40.939023Z",
     "iopub.status.busy": "2022-04-10T14:05:40.938367Z",
     "iopub.status.idle": "2022-04-10T14:05:40.941210Z",
     "shell.execute_reply": "2022-04-10T14:05:40.941721Z",
     "shell.execute_reply.started": "2022-04-10T14:03:13.094641Z"
    },
    "papermill": {
     "duration": 0.115447,
     "end_time": "2022-04-10T14:05:40.941878",
     "exception": false,
     "start_time": "2022-04-10T14:05:40.826431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>countv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like</td>\n",
       "      <td>9236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel</td>\n",
       "      <td>8674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year</td>\n",
       "      <td>8313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>know</td>\n",
       "      <td>6011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want</td>\n",
       "      <td>5684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>life</td>\n",
       "      <td>5592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>get</td>\n",
       "      <td>5344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>time</td>\n",
       "      <td>5271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>even</td>\n",
       "      <td>4393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>friend</td>\n",
       "      <td>4227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corpus  countv\n",
       "0    like    9236\n",
       "1    feel    8674\n",
       "2    year    8313\n",
       "3    know    6011\n",
       "4    want    5684\n",
       "5    life    5592\n",
       "6     get    5344\n",
       "7    time    5271\n",
       "8    even    4393\n",
       "9  friend    4227"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(corpus)\n",
    "most_common = counter.most_common(10)\n",
    "most_common = pd.DataFrame(most_common,columns = ['corpus','countv'])\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a09871e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:41.017225Z",
     "iopub.status.busy": "2022-04-10T14:05:41.016572Z",
     "iopub.status.idle": "2022-04-10T14:05:41.019282Z",
     "shell.execute_reply": "2022-04-10T14:05:41.018854Z",
     "shell.execute_reply.started": "2022-04-10T14:03:13.233189Z"
    },
    "papermill": {
     "duration": 0.043362,
     "end_time": "2022-04-10T14:05:41.019417",
     "exception": false,
     "start_time": "2022-04-10T14:05:40.976055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngrams(review, n, g):\n",
    "    vec = CountVectorizer(ngram_range=(g, g)).fit(review)\n",
    "    bag_of_words = vec.transform(review) #sparse matrix of count_vectorizer\n",
    "    sum_words = bag_of_words.sum(axis=0) #total number of words\n",
    "    sum_words = np.array(sum_words)[0].tolist() #convert to list\n",
    "    words_freq = [(word, sum_words[idx]) for word, idx in vec.vocabulary_.items()] #get word freqency for word location in count vec\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) #key is used to perform sorting using word_freqency \n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61ddd20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:41.095893Z",
     "iopub.status.busy": "2022-04-10T14:05:41.095386Z",
     "iopub.status.idle": "2022-04-10T14:05:42.249408Z",
     "shell.execute_reply": "2022-04-10T14:05:42.248456Z",
     "shell.execute_reply.started": "2022-04-10T14:03:13.251211Z"
    },
    "papermill": {
     "duration": 1.194871,
     "end_time": "2022-04-10T14:05:42.249552",
     "exception": false,
     "start_time": "2022-04-10T14:05:41.054681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splitting into train and test\n",
    "train, test= train_test_split(data, test_size=0.2, random_state=42)\n",
    "Xtrain, ytrain = train['Text_data'], train['Label']\n",
    "Xtest, ytest = test['Text_data'], test['Label']\n",
    "#Vectorizing data\n",
    "\n",
    "tfidf_vect = TfidfVectorizer() #tfidfVectorizer\n",
    "Xtrain_tfidf = tfidf_vect.fit_transform(Xtrain)\n",
    "Xtest_tfidf = tfidf_vect.transform(Xtest)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer() # CountVectorizer\n",
    "Xtrain_count = count_vect.fit_transform(Xtrain)\n",
    "Xtest_count = count_vect.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "232fc86d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:42.342185Z",
     "iopub.status.busy": "2022-04-10T14:05:42.336930Z",
     "iopub.status.idle": "2022-04-10T14:05:42.497458Z",
     "shell.execute_reply": "2022-04-10T14:05:42.496955Z",
     "shell.execute_reply.started": "2022-04-10T14:03:14.454472Z"
    },
    "papermill": {
     "duration": 0.213174,
     "end_time": "2022-04-10T14:05:42.497599",
     "exception": false,
     "start_time": "2022-04-10T14:05:42.284425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "corpus = [word for text in df['cleaned_Text_data'] for word in text.split()]\n",
    "count_words = Counter(corpus)\n",
    "sorted_words = count_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64ce16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:42.611574Z",
     "iopub.status.busy": "2022-04-10T14:05:42.575491Z",
     "iopub.status.idle": "2022-04-10T14:05:42.709617Z",
     "shell.execute_reply": "2022-04-10T14:05:42.710222Z",
     "shell.execute_reply.started": "2022-04-10T14:03:14.627239Z"
    },
    "papermill": {
     "duration": 0.177574,
     "end_time": "2022-04-10T14:05:42.710430",
     "exception": false,
     "start_time": "2022-04-10T14:05:42.532856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[467, 133, 1011, 1717, 1805, 41, 517, 58, 18, 4, 48, 86, 5, 130, 191, 596, 261, 24, 45, 50, 104, 12, 57, 61, 10, 126, 2030, 109, 184, 2628, 427, 104, 346, 338, 136, 362, 461, 10, 1384, 482, 293, 673, 34, 384, 43, 753, 3, 124, 191, 249]]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "Text_data_int = []\n",
    "for text in df['cleaned_Text_data']:\n",
    "    r = [vocab_to_int[word] for word in text.split()]\n",
    "    Text_data_int.append(r)\n",
    "\n",
    "print(Text_data_int[:1])\n",
    "df['Text_data int'] = Text_data_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc546538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:42.801913Z",
     "iopub.status.busy": "2022-04-10T14:05:42.801075Z",
     "iopub.status.idle": "2022-04-10T14:05:42.804002Z",
     "shell.execute_reply": "2022-04-10T14:05:42.804484Z",
     "shell.execute_reply.started": "2022-04-10T14:03:14.775263Z"
    },
    "papermill": {
     "duration": 0.057577,
     "end_time": "2022-04-10T14:05:42.804623",
     "exception": false,
     "start_time": "2022-04-10T14:05:42.747046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_Text_data</th>\n",
       "      <th>Text_data int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Waiting for my mind to have a breakdown once t...</td>\n",
       "      <td>1</td>\n",
       "      <td>waiting mind breakdown “new year” feeling isn’...</td>\n",
       "      <td>[467, 133, 1011, 1717, 1805, 41, 517, 58, 18, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My new years resolution : I'm gonna get my ass...</td>\n",
       "      <td>1</td>\n",
       "      <td>new years resolution im gonna get ass therapis...</td>\n",
       "      <td>[13, 16, 1105, 1, 189, 9, 619, 982, 1623, 8, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New year : Somone else Feeling like 2020 will ...</td>\n",
       "      <td>1</td>\n",
       "      <td>new year somone else feeling like 2020 last ye...</td>\n",
       "      <td>[13, 10, 3358, 86, 41, 2, 70, 61, 10, 1021, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My story I guess : Hi, Im from Germany and my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>story guess hi im germany english mostly self ...</td>\n",
       "      <td>[367, 182, 707, 1, 1583, 995, 674, 159, 1169, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat in the dark and cried myself going into th...</td>\n",
       "      <td>1</td>\n",
       "      <td>sat dark cried going new year great start 2020</td>\n",
       "      <td>[725, 332, 351, 23, 13, 10, 176, 109, 70]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text_data  Label  \\\n",
       "0  Waiting for my mind to have a breakdown once t...      1   \n",
       "1  My new years resolution : I'm gonna get my ass...      1   \n",
       "2  New year : Somone else Feeling like 2020 will ...      1   \n",
       "3  My story I guess : Hi, Im from Germany and my ...      1   \n",
       "4  Sat in the dark and cried myself going into th...      1   \n",
       "\n",
       "                                   cleaned_Text_data  \\\n",
       "0  waiting mind breakdown “new year” feeling isn’...   \n",
       "1  new years resolution im gonna get ass therapis...   \n",
       "2  new year somone else feeling like 2020 last ye...   \n",
       "3  story guess hi im germany english mostly self ...   \n",
       "4     sat dark cried going new year great start 2020   \n",
       "\n",
       "                                       Text_data int  \n",
       "0  [467, 133, 1011, 1717, 1805, 41, 517, 58, 18, ...  \n",
       "1  [13, 16, 1105, 1, 189, 9, 619, 982, 1623, 8, 3...  \n",
       "2  [13, 10, 3358, 86, 41, 2, 70, 61, 10, 1021, 11...  \n",
       "3  [367, 182, 707, 1, 1583, 995, 674, 159, 1169, ...  \n",
       "4          [725, 332, 351, 23, 13, 10, 176, 109, 70]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def condition(x):\n",
    "    if x=='moderate':\n",
    "        return 1\n",
    "    elif x=='severe':\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "df['Label'] = df['Label'].apply(condition)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dcc238d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:42.891566Z",
     "iopub.status.busy": "2022-04-10T14:05:42.890805Z",
     "iopub.status.idle": "2022-04-10T14:05:42.893711Z",
     "shell.execute_reply": "2022-04-10T14:05:42.894145Z",
     "shell.execute_reply.started": "2022-04-10T14:03:14.799426Z"
    },
    "papermill": {
     "duration": 0.054717,
     "end_time": "2022-04-10T14:05:42.894263",
     "exception": false,
     "start_time": "2022-04-10T14:05:42.839546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_Text_data</th>\n",
       "      <th>Text_data int</th>\n",
       "      <th>Text_data len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Waiting for my mind to have a breakdown once t...</td>\n",
       "      <td>1</td>\n",
       "      <td>waiting mind breakdown “new year” feeling isn’...</td>\n",
       "      <td>[467, 133, 1011, 1717, 1805, 41, 517, 58, 18, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My new years resolution : I'm gonna get my ass...</td>\n",
       "      <td>1</td>\n",
       "      <td>new years resolution im gonna get ass therapis...</td>\n",
       "      <td>[13, 16, 1105, 1, 189, 9, 619, 982, 1623, 8, 3...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New year : Somone else Feeling like 2020 will ...</td>\n",
       "      <td>1</td>\n",
       "      <td>new year somone else feeling like 2020 last ye...</td>\n",
       "      <td>[13, 10, 3358, 86, 41, 2, 70, 61, 10, 1021, 11...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My story I guess : Hi, Im from Germany and my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>story guess hi im germany english mostly self ...</td>\n",
       "      <td>[367, 182, 707, 1, 1583, 995, 674, 159, 1169, ...</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat in the dark and cried myself going into th...</td>\n",
       "      <td>1</td>\n",
       "      <td>sat dark cried going new year great start 2020</td>\n",
       "      <td>[725, 332, 351, 23, 13, 10, 176, 109, 70]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text_data  Label  \\\n",
       "0  Waiting for my mind to have a breakdown once t...      1   \n",
       "1  My new years resolution : I'm gonna get my ass...      1   \n",
       "2  New year : Somone else Feeling like 2020 will ...      1   \n",
       "3  My story I guess : Hi, Im from Germany and my ...      1   \n",
       "4  Sat in the dark and cried myself going into th...      1   \n",
       "\n",
       "                                   cleaned_Text_data  \\\n",
       "0  waiting mind breakdown “new year” feeling isn’...   \n",
       "1  new years resolution im gonna get ass therapis...   \n",
       "2  new year somone else feeling like 2020 last ye...   \n",
       "3  story guess hi im germany english mostly self ...   \n",
       "4     sat dark cried going new year great start 2020   \n",
       "\n",
       "                                       Text_data int  Text_data len  \n",
       "0  [467, 133, 1011, 1717, 1805, 41, 517, 58, 18, ...             50  \n",
       "1  [13, 16, 1105, 1, 189, 9, 619, 982, 1623, 8, 3...             26  \n",
       "2  [13, 10, 3358, 86, 41, 2, 70, 61, 10, 1021, 11...             18  \n",
       "3  [367, 182, 707, 1, 1583, 995, 674, 159, 1169, ...            296  \n",
       "4          [725, 332, 351, 23, 13, 10, 176, 109, 70]              9  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_data_len = [len(x) for x in Text_data_int]\n",
    "df['Text_data len'] = Text_data_len\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e453256",
   "metadata": {
    "papermill": {
     "duration": 0.036473,
     "end_time": "2022-04-10T14:05:42.966691",
     "exception": false,
     "start_time": "2022-04-10T14:05:42.930218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Padding/truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c1df89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:43.046302Z",
     "iopub.status.busy": "2022-04-10T14:05:43.045559Z",
     "iopub.status.idle": "2022-04-10T14:05:43.369387Z",
     "shell.execute_reply": "2022-04-10T14:05:43.370207Z",
     "shell.execute_reply.started": "2022-04-10T14:03:14.821514Z"
    },
    "papermill": {
     "duration": 0.366735,
     "end_time": "2022-04-10T14:05:43.370431",
     "exception": false,
     "start_time": "2022-04-10T14:05:43.003696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  467  133 1011 1717\n",
      " 1805   41  517   58   18    4   48   86    5  130  191  596  261   24\n",
      "   45   50  104   12   57   61   10  126 2030  109  184 2628  427  104\n",
      "  346  338  136  362  461   10 1384  482  293  673   34  384   43  753\n",
      "    3  124  191  249]\n"
     ]
    }
   ],
   "source": [
    "def Padding(Text_data_int, seq_len):\n",
    "    features = np.zeros((len(Text_data_int), seq_len), dtype = int)\n",
    "    for i, Text_data in enumerate(Text_data_int):\n",
    "        if len(Text_data) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(Text_data)))\n",
    "            new = zeros + Text_data\n",
    "        else:\n",
    "            new = Text_data[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "            \n",
    "    return features\n",
    "features = Padding(Text_data_int, 200)\n",
    "print(features[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8a1cfc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:43.459747Z",
     "iopub.status.busy": "2022-04-10T14:05:43.458772Z",
     "iopub.status.idle": "2022-04-10T14:05:43.472861Z",
     "shell.execute_reply": "2022-04-10T14:05:43.472328Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.152239Z"
    },
    "papermill": {
     "duration": 0.061891,
     "end_time": "2022-04-10T14:05:43.472982",
     "exception": false,
     "start_time": "2022-04-10T14:05:43.411091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_remain, y_train, y_remain = train_test_split(features, df['Label'].to_numpy(), test_size=0.2, random_state=1)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1a0070a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:43.557098Z",
     "iopub.status.busy": "2022-04-10T14:05:43.556458Z",
     "iopub.status.idle": "2022-04-10T14:05:43.606699Z",
     "shell.execute_reply": "2022-04-10T14:05:43.607065Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.170710Z"
    },
    "papermill": {
     "duration": 0.097479,
     "end_time": "2022-04-10T14:05:43.607217",
     "exception": false,
     "start_time": "2022-04-10T14:05:43.509738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,   111,  1842,   131],\n",
      "        [    0,     0,     0,  ...,   272,   229,    48],\n",
      "        [    0,     0,     0,  ...,    60, 12273,   131],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,    59,  1899,   287],\n",
      "        [    0,     0,     0,  ...,   124,     2,   443],\n",
      "        [    0,     0,     0,  ...,  2289,  9602,    22]])\n",
      "Sample input: \n",
      " tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "# create tensor dataset\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c303e49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:43.684462Z",
     "iopub.status.busy": "2022-04-10T14:05:43.683658Z",
     "iopub.status.idle": "2022-04-10T14:05:43.695113Z",
     "shell.execute_reply": "2022-04-10T14:05:43.694711Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.188660Z"
    },
    "papermill": {
     "duration": 0.050916,
     "end_time": "2022-04-10T14:05:43.695223",
     "exception": false,
     "start_time": "2022-04-10T14:05:43.644307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "802608db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:43.827309Z",
     "iopub.status.busy": "2022-04-10T14:05:43.826746Z",
     "iopub.status.idle": "2022-04-10T14:05:43.830375Z",
     "shell.execute_reply": "2022-04-10T14:05:43.831279Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.202287Z"
    },
    "papermill": {
     "duration": 0.099212,
     "end_time": "2022-04-10T14:05:43.831492",
     "exception": false,
     "start_time": "2022-04-10T14:05:43.732280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7a40209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:43.912133Z",
     "iopub.status.busy": "2022-04-10T14:05:43.911296Z",
     "iopub.status.idle": "2022-04-10T14:05:43.988715Z",
     "shell.execute_reply": "2022-04-10T14:05:43.989133Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.214365Z"
    },
    "papermill": {
     "duration": 0.119096,
     "end_time": "2022-04-10T14:05:43.989277",
     "exception": false,
     "start_time": "2022-04-10T14:05:43.870181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Waiting for my mind to have a breakdown once t...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My new years resolution : I'm gonna get my ass...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New year : Somone else Feeling like 2020 will ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My story I guess : Hi, Im from Germany and my ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat in the dark and cried myself going into th...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text_data     Label\n",
       "0  Waiting for my mind to have a breakdown once t...  moderate\n",
       "1  My new years resolution : I'm gonna get my ass...  moderate\n",
       "2  New year : Somone else Feeling like 2020 will ...  moderate\n",
       "3  My story I guess : Hi, Im from Germany and my ...  moderate\n",
       "4  Sat in the dark and cried myself going into th...  moderate"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_csv = '../input/depression/depression_train.csv'\n",
    "df = pd.read_csv(base_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9019a869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:44.072650Z",
     "iopub.status.busy": "2022-04-10T14:05:44.071907Z",
     "iopub.status.idle": "2022-04-10T14:05:44.086538Z",
     "shell.execute_reply": "2022-04-10T14:05:44.086925Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.297910Z"
    },
    "papermill": {
     "duration": 0.059512,
     "end_time": "2022-04-10T14:05:44.087058",
     "exception": false,
     "start_time": "2022-04-10T14:05:44.027546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (6668,)\n",
      "shape of test data is (2223,)\n"
     ]
    }
   ],
   "source": [
    "X,y = df['Text_data'].values,df['Label'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ede8337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:44.165731Z",
     "iopub.status.busy": "2022-04-10T14:05:44.164938Z",
     "iopub.status.idle": "2022-04-10T14:05:44.175938Z",
     "shell.execute_reply": "2022-04-10T14:05:44.175542Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.321261Z"
    },
    "papermill": {
     "duration": 0.051255,
     "end_time": "2022-04-10T14:05:44.176040",
     "exception": false,
     "start_time": "2022-04-10T14:05:44.124785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def tockenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0bef751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:44.292139Z",
     "iopub.status.busy": "2022-04-10T14:05:44.259110Z",
     "iopub.status.idle": "2022-04-10T14:05:55.457739Z",
     "shell.execute_reply": "2022-04-10T14:05:55.458329Z",
     "shell.execute_reply.started": "2022-04-10T14:03:15.337831Z"
    },
    "papermill": {
     "duration": 11.245023,
     "end_time": "2022-04-10T14:05:55.458543",
     "exception": false,
     "start_time": "2022-04-10T14:05:44.213520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcd0a392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:55.542069Z",
     "iopub.status.busy": "2022-04-10T14:05:55.541264Z",
     "iopub.status.idle": "2022-04-10T14:05:55.637932Z",
     "shell.execute_reply": "2022-04-10T14:05:55.638405Z",
     "shell.execute_reply.started": "2022-04-10T14:03:27.392458Z"
    },
    "papermill": {
     "duration": 0.141089,
     "end_time": "2022-04-10T14:05:55.638572",
     "exception": false,
     "start_time": "2022-04-10T14:05:55.497483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "#we have very less number of reviews with length > 500.\n",
    "#So we will consideronly those below it.\n",
    "x_train_pad = padding_(x_train,500)\n",
    "x_test_pad = padding_(x_test,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d24f1745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:55.722305Z",
     "iopub.status.busy": "2022-04-10T14:05:55.721489Z",
     "iopub.status.idle": "2022-04-10T14:05:55.728803Z",
     "shell.execute_reply": "2022-04-10T14:05:55.729423Z",
     "shell.execute_reply.started": "2022-04-10T14:03:27.498731Z"
    },
    "papermill": {
     "duration": 0.052504,
     "end_time": "2022-04-10T14:05:55.729616",
     "exception": false,
     "start_time": "2022-04-10T14:05:55.677112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 500])\n",
      "Sample input: \n",
      " tensor([[  0,   0,   0,  ..., 206,   3,   5],\n",
      "        [  0,   0,   0,  ..., 191,  36,  49],\n",
      "        [  0,   0,   0,  ...,  25,  22,   7],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ..., 187, 265,  39],\n",
      "        [  0,   0,   0,  ..., 234,   1, 460],\n",
      "        [  0,   0,   0,  ..., 253, 861, 336]])\n",
      "Sample input: \n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e7d4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:05:55.813573Z",
     "iopub.status.busy": "2022-04-10T14:05:55.812750Z",
     "iopub.status.idle": "2022-04-10T14:06:00.869772Z",
     "shell.execute_reply": "2022-04-10T14:06:00.873041Z",
     "shell.execute_reply.started": "2022-04-10T14:03:27.514972Z"
    },
    "papermill": {
     "duration": 5.104492,
     "end_time": "2022-04-10T14:06:00.873292",
     "exception": false,
     "start_time": "2022-04-10T14:05:55.768800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(1001, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e82001e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:06:00.970391Z",
     "iopub.status.busy": "2022-04-10T14:06:00.968662Z",
     "iopub.status.idle": "2022-04-10T14:06:00.970980Z",
     "shell.execute_reply": "2022-04-10T14:06:00.971420Z",
     "shell.execute_reply.started": "2022-04-10T14:03:27.533878Z"
    },
    "papermill": {
     "duration": 0.05036,
     "end_time": "2022-04-10T14:06:00.971563",
     "exception": false,
     "start_time": "2022-04-10T14:06:00.921203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0f31e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:06:01.060592Z",
     "iopub.status.busy": "2022-04-10T14:06:01.059825Z",
     "iopub.status.idle": "2022-04-10T14:06:01.061753Z",
     "shell.execute_reply": "2022-04-10T14:06:01.062189Z",
     "shell.execute_reply.started": "2022-04-10T14:03:27.839746Z"
    },
    "papermill": {
     "duration": 0.04941,
     "end_time": "2022-04-10T14:06:01.062363",
     "exception": false,
     "start_time": "2022-04-10T14:06:01.012953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n",
    "                         if preprocess_string(word) in vocab.keys()])\n",
    "        word_seq = np.expand_dims(word_seq,axis=0)\n",
    "        pad =  torch.from_numpy(padding_(word_seq,500))\n",
    "        inputs = pad.to(device)\n",
    "        batch_size = 1\n",
    "        h = model.init_hidden(batch_size)\n",
    "        h = tuple([each.data for each in h])\n",
    "        output, h = model(inputs, h)\n",
    "        return(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d398e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:06:01.144794Z",
     "iopub.status.busy": "2022-04-10T14:06:01.144006Z",
     "iopub.status.idle": "2022-04-10T14:06:02.033042Z",
     "shell.execute_reply": "2022-04-10T14:06:02.033447Z",
     "shell.execute_reply.started": "2022-04-10T14:03:27.853154Z"
    },
    "papermill": {
     "duration": 0.932494,
     "end_time": "2022-04-10T14:06:02.033615",
     "exception": false,
     "start_time": "2022-04-10T14:06:01.101121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just got let go from my job yesterday and I am so depressed right now. : Hey everyone. I’m pretty new to this community but from what I have read it seems really supportive. I was already kind of experiencing a depressive episode but this was just the cherry on top. I’m too depressed to cry if that’s even possible. Do you guys have any tips on how to feel better? I’ve been applying to new jobs all day. thanks in advance ❤️\n",
      "======================================================================\n",
      "Actual sentiment is  : moderate\n",
      "======================================================================\n",
      "Predicted sentiment is positive with a probability of 0.5080389976501465\n"
     ]
    }
   ],
   "source": [
    "index = 30\n",
    "print(df['Text_data'][index])\n",
    "print('='*70)\n",
    "print(f'Actual sentiment is  : {df[\"Label\"][index]}')\n",
    "print('='*70)\n",
    "pro = predict_text(df['Text_data'][index])\n",
    "status = \"positive\" if pro > 0.5 else \"negative\"\n",
    "pro = (1 - pro) if status == \"negative\" else pro\n",
    "print(f'Predicted sentiment is {status} with a probability of {pro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd999bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:06:02.120953Z",
     "iopub.status.busy": "2022-04-10T14:06:02.117712Z",
     "iopub.status.idle": "2022-04-10T14:06:02.138586Z",
     "shell.execute_reply": "2022-04-10T14:06:02.137975Z",
     "shell.execute_reply.started": "2022-04-10T14:03:27.880374Z"
    },
    "papermill": {
     "duration": 0.065839,
     "end_time": "2022-04-10T14:06:02.138743",
     "exception": false,
     "start_time": "2022-04-10T14:06:02.072904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 was my worst year with 2 depression crisis. I'm happy it ended but so afraid of what 2020 will bring. : This years was rough. It started on the NYE with my puppy almost dying from the fireworks. He literally shat all over myself. In may I had my first terrible depression and anxiety crisis and had to be away from my internship for 2 weeks. Then in June I went through the first real loss of my life. My dear uncle died from a heart attack al of sudden. Even though all of this, I got promoted on my job. It was a complicated time dealing with new responsibilities and grief. I fell on depression again and this time was worse  I literally wanted to die. Even hurted myself. This made me get away from work again but now for 2 months. I had great support from family and the company I work for, but it was hard to get back on my feet again. During all of these I had to leave university for the year as I couldn't deal with the pression at the time. The thing is I study Industrial Engineer and I'm working as a Marketing E   Commerce Analyst for Dell.com.br. I don't know if I want to keep with my engineering program, but I'm not that away from finishing it. \n",
      "\n",
      "Long story short, I'm lost and don't know what to expect from 2020. I'm just relieved 2019 ended, but I'm really afraid of what 2020 will bring.\n",
      "======================================================================\n",
      "Actual sentiment is  : moderate\n",
      "predicted sentiment is positive with a probability of 0.5086900591850281\n"
     ]
    }
   ],
   "source": [
    "index = 32\n",
    "print(df['Text_data'][index])\n",
    "print('='*70)\n",
    "print(f'Actual sentiment is  : {df[\"Label\"][index]}')\n",
    "pro = predict_text(df['Text_data'][index])\n",
    "status = \"positive\" if pro > 0.5 else \"negative\"\n",
    "pro = (1 - pro) if status == \"negative\" else pro\n",
    "print(f'predicted sentiment is {status} with a probability of {pro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9dcdc",
   "metadata": {
    "papermill": {
     "duration": 0.039155,
     "end_time": "2022-04-10T14:06:02.217960",
     "exception": false,
     "start_time": "2022-04-10T14:06:02.178805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f8f3f",
   "metadata": {
    "papermill": {
     "duration": 0.038794,
     "end_time": "2022-04-10T14:06:02.296905",
     "exception": false,
     "start_time": "2022-04-10T14:06:02.258111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 58.343217,
   "end_time": "2022-04-10T14:06:05.256726",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-10T14:05:06.913509",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
